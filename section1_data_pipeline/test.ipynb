{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2d11395-598d-44fc-a914-9d5289495df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f0568332-bc00-4157-8263-7a7bdb18eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "caedc160-dac8-49aa-9900-b6a23947fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_and_drop_nan_names(data_path: str):\n",
    "    # read csv file\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "        data_path + \"/raw_data/dataset2.csv\")\n",
    "    # drop names with nan\n",
    "    df = df.na.drop(subset=[\"name\"])\n",
    "    df.coalesce(1).write.format('com.databricks.spark.csv').save(data_path + \"/raw_data/dataset_v1.csv\",header = 'true')\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "09197fe1-5e6d-4077-bfcd-afba0141a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_names(data_path: str):\n",
    "    # read processed csv file with dropped nan names\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "        data_path + \"/raw_data/dataset_v1.csv\")\n",
    "    # split name into first_name, last_name\n",
    "    split_name_col = F.split(df['name'], ' ')\n",
    "    df = df.withColumn('first_name', split_name_col.getItem(0))\n",
    "    df = df.withColumn('last_name', split_name_col.getItem(1))\n",
    "\n",
    "    # save only the first_name amd last_name columns\n",
    "    df = df.select(\"first_name\", \"last_name\")\n",
    "    \n",
    "    df.coalesce(1).write.format('com.databricks.spark.csv').save(data_path + \"/raw_data/dataset_name.csv\",header = 'true')\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d5bb432-11c3-4499-a6c1-c284aa592b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prepended_zeroes_add_above_100(data_path: str):\n",
    "    # read processed csv file with dropped nan names\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "        data_path + \"/raw_data/dataset_v1.csv\")\n",
    "\n",
    "    # casting price as string\n",
    "    df = df.withColumn(\"price\", df[\"price\"].cast(StringType()))\n",
    "    # remove trailing zeros\n",
    "    df = df.withColumn('price', F.regexp_replace('price', r'^[0]*', ''))\n",
    "    # add above_100\n",
    "    df = df.withColumn('above_100', F.when(\n",
    "        F.col('price') > 100, True).otherwise(False))\n",
    "\n",
    "    # save only the price amd above_100 columns\n",
    "    df = df.select(\"price\", \"above_100\")\n",
    "    df.coalesce(1).write.format('com.databricks.spark.csv').save(data_path + \"/raw_data/dataset_price.csv\",header = 'true')\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dce2a0f5-04e0-40a8-87a2-826aeb8a4fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cols_and_save_file(data_path: str):\n",
    "    # read processed csv file with dropped nan names\n",
    "    df_name = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "        data_path + \"/raw_data/dataset_name.csv\")\n",
    "    df_price = spark.read.format(\"csv\").option(\"header\", \"true\").load(\n",
    "        data_path + \"/raw_data/dataset_price.csv\")\n",
    "\n",
    "    # adding dumming id1 and id2 to merge the two dataframes\n",
    "    df_name = df_name.withColumn(\"id1\", F.monotonically_increasing_id())\n",
    "    df_price = df_price.withColumn(\"id2\", F.monotonically_increasing_id())\n",
    "\n",
    "    # merge columns in df_name and df_price\n",
    "    df_final = df_name.join(df_price,F.col(\"id1\") == F.col(\"id2\"),\"inner\").drop(\"id1\",\"id2\")\n",
    "    \n",
    "    # save processed dataset\n",
    "    df_final.coalesce(1).write.format('com.databricks.spark.csv').save(\n",
    "        data_path + \"/processed_data/dataset.csv\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55f9b774-d01b-4c33-8d5c-6a09b4d6588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/anshu/Work/Code/Projects/data_engineering/section1_data_pipeline/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82e64d3b-febd-4f54-b878-31790038d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file_and_drop_nan_names(DATA_PATH)\n",
    "split_names(DATA_PATH)\n",
    "remove_prepended_zeroes_add_above_100(DATA_PATH)\n",
    "merge_cols_and_save_file(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d67e9-2dc2-40c7-a9e6-d938a8549ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
